{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "937236a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.utils import class_weight\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "da9bb0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Neural Network Model\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "# Function to preprocess data (convert to tensors)\n",
    "def preprocess_data(train_data, batch_size=32):\n",
    "    # remove null value \n",
    "    train.rename(columns={'bad_flag': 'target'}, inplace = True)\n",
    "    train.dropna(subset=['target'], inplace=True)\n",
    "    train['emp_length'] =train['emp_length'].fillna('UNKNOWN')\n",
    "    train['percent_bc_gt_75'] = train['percent_bc_gt_75'].fillna(-1)\n",
    "    train['mths_since_recent_inq'] = train['mths_since_recent_inq'].fillna(-1)\n",
    "    train['revol_util'] = train['revol_util'].fillna(-1)\n",
    "    train['total_bc_limit'] = train['total_bc_limit'].fillna(-1)\n",
    "    train['mths_since_last_major_derog'] = train['mths_since_last_major_derog'].fillna(-1)\n",
    "    train['tot_hi_cred_lim'] = train['tot_hi_cred_lim'].fillna(-1)\n",
    "    train['bc_util'] = train['bc_util'].fillna(-1)\n",
    "    train['tot_cur_bal'] = train['tot_cur_bal'].fillna(-1)\n",
    "\n",
    "    train['desc_clean'] = train['desc'].fillna('').apply(clean_text)\n",
    "\n",
    "    train['desc_clean'] = train['desc'].fillna('')\n",
    "\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')  # Lightweight version of BERT for sentence embeddings\n",
    "\n",
    "    # Generate embeddings for each sentence\n",
    "    embeddings = model.encode(train['desc_clean'].tolist(), show_progress_bar=True)\n",
    "\n",
    "\n",
    "    # Check the shape of the embeddings\n",
    "    print(embeddings.shape)  # Output: (num_samples, embedding_dimension)\n",
    "    embedding_df = pd.DataFrame(embeddings, columns=[f\"emb_{i}\" for i in range(embeddings.shape[1])])\n",
    "\n",
    "    train_with_embeddings = pd.concat([train.reset_index(drop=True), embedding_df.reset_index(drop=True)], axis=1)\n",
    "    train_with_embeddings.drop(['desc'],axis = 1, inplace = True)   \n",
    "    \n",
    "    \n",
    "    exclude_cols = ['id', 'member_id']  \n",
    "    target_col = 'target'\n",
    "    # Find columns with missing values\n",
    "    columns_with_nan = train_with_embeddings.columns[train_with_embeddings.isnull().any()]\n",
    "\n",
    "    # Print the column names\n",
    "    print(\"Columns with missing values:\")\n",
    "    print(columns_with_nan)\n",
    "    features = train_with_embeddings.drop(columns=exclude_cols+ [target_col], errors='ignore')\n",
    "    target = train_with_embeddings[target_col]\n",
    "    \n",
    "    # Encode non-numeric columns\n",
    "    non_numeric_cols = features.select_dtypes(include=['object', 'category']).columns\n",
    "    label_encoder = LabelEncoder()\n",
    "    for col in non_numeric_cols:\n",
    "        features[col] = label_encoder.fit_transform(features[col].astype(str))\n",
    "    train_data, val_data, train_labels, val_labels = train_test_split(\n",
    "    features, target, test_size=0.2, random_state=78)\n",
    "    \n",
    "    # Scale data\n",
    "    scaler = StandardScaler()\n",
    "    train_data = scaler.fit_transform(train_data)\n",
    "    val_data = scaler.transform(val_data)\n",
    "    # Convert target to a single binary column\n",
    "    train_labels = train_labels.astype(int)  # Ensure target is integer\n",
    "    val_labels = val_labels.astype(int)\n",
    "    X_train = torch.tensor(train_data, dtype=torch.float32)\n",
    "    y_train = torch.tensor(train_labels.values, dtype=torch.float32).unsqueeze(1)\n",
    "    X_val = torch.tensor(val_data, dtype=torch.float32)\n",
    "    y_val = torch.tensor(val_labels.values, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    val_dataset = TensorDataset(X_val, y_val)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    input_size = X_train.shape[1]\n",
    "    print('Finish preprocessing')\n",
    "\n",
    "    return train_loader, val_loader, input_size, scaler\n",
    "\n",
    "# Function to train the model with early stopping\n",
    "def train_model(train_loader, val_loader, input_size, hidden_size=64, lr=0.01, patience=5, num_epochs=50):\n",
    "    print('Start Training')\n",
    "    # Model, Loss, Optimizer\n",
    "    model = NeuralNet(input_size, hidden_size)\n",
    "\n",
    "    criterion = nn.BCELoss()  # Binary Cross-Entropy Loss\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "    scheduler = StepLR(optimizer, step_size=5, gamma=0.1)  # Learning Rate Scheduler\n",
    "\n",
    "    # Early Stopping Parameters\n",
    "    patience = 5  # Number of epochs to wait for improvement\n",
    "    best_roc_auc = 0.0\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    # Training Loop\n",
    "    num_epochs = 50\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training Phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Validation Phase\n",
    "        model.eval()\n",
    "        val_preds = []\n",
    "        val_targets = []\n",
    "        val_loss = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for val_X, val_y in val_loader:\n",
    "                val_outputs = model(val_X)\n",
    "                loss = criterion(val_outputs, val_y)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                val_preds.extend(val_outputs.squeeze().cpu().numpy())\n",
    "                val_targets.extend(val_y.squeeze().cpu().numpy())\n",
    "\n",
    "        # Calculate ROC-AUC\n",
    "        roc_auc = roc_auc_score(val_targets, val_preds)\n",
    "\n",
    "        # Step the scheduler\n",
    "        scheduler.step()\n",
    "\n",
    "        # Print metrics\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "              f\"Train Loss: {train_loss/len(train_loader):.4f}, \"\n",
    "              f\"Val Loss: {val_loss/len(val_loader):.4f}, \"\n",
    "              f\"ROC-AUC: {roc_auc:.4f}, \"\n",
    "              f\"Learning Rate: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "\n",
    "        # Early Stopping Check\n",
    "        if roc_auc > best_roc_auc:\n",
    "            best_roc_auc = roc_auc\n",
    "            epochs_no_improve = 0\n",
    "            torch.save(model.state_dict(), 'best_model.pth')  # Save the best model\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"Early stopping triggered after {epoch+1} epochs.\")\n",
    "            break\n",
    "\n",
    "    # Load the Best Model\n",
    "    model.load_state_dict(torch.load('best_model.pth'))\n",
    "\n",
    "    # Final ROC-AUC\n",
    "    model.eval()\n",
    "    final_preds = []\n",
    "    final_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for val_X, val_y in val_loader:\n",
    "            val_outputs = model(val_X)\n",
    "            final_preds.extend(val_outputs.squeeze().cpu().numpy())\n",
    "            final_targets.extend(val_y.squeeze().cpu().numpy())\n",
    "\n",
    "    final_roc_auc = roc_auc_score(final_targets, final_preds)\n",
    "    print(f\"Final Validation ROC-AUC: {final_roc_auc:.4f}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Function to predict on test data\n",
    "def predict(model, test_data):\n",
    "    model.eval()\n",
    "    X_test = torch.tensor(test_data, dtype=torch.float32)\n",
    "    test_loader = DataLoader(X_test, batch_size=32, shuffle=False)\n",
    "\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch_X in test_loader:\n",
    "            outputs = model(batch_X)\n",
    "            predictions.extend(outputs.squeeze().cpu().numpy())\n",
    "\n",
    "    return np.array(predictions)\n",
    "\n",
    "# Function to load best model\n",
    "def load_best_model(input_size, hidden_size, model_path='best_model.pth'):\n",
    "    model = NeuralNet(input_size, hidden_size)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "# Custom cleaning function with lemmatization\n",
    "def clean_text(text):\n",
    "    # Initialize lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = re.sub(r'Borrower added on \\d{2}/\\d{2}/\\d{2}', '', text)  # Remove \"Borrower added on XX/XX/XX\"\n",
    "    text = re.sub(r'<.*?>', '', text)  # Remove HTML tags\n",
    "    text = re.sub(r'[^a-zA-Z]', ' ', text)  # Remove non-alphabetic characters\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    stop_words = set(stopwords.words('english'))  # Stopwords\n",
    "    custom_stopwords = ['also', 'one', 'thank', 'added', 'help']  # Custom stopwords\n",
    "    stop_words.update(custom_stopwords)\n",
    "\n",
    "    # Lemmatize words and remove stopwords\n",
    "    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words])\n",
    "    return text\n",
    "\n",
    "def preprocess_test_data(test_data, scaler, exclude_cols=['id', 'member_id'], text_column='desc'):\n",
    "    # Rename columns if needed\n",
    "    test_data.rename(columns={'bad_flag': 'target'}, inplace=True)\n",
    "\n",
    "    # Handle missing values in the test data\n",
    "    test_data['emp_length'] = test_data['emp_length'].fillna('UNKNOWN')\n",
    "    test_data['percent_bc_gt_75'] = test_data['percent_bc_gt_75'].fillna(-1)\n",
    "    test_data['mths_since_recent_inq'] = test_data['mths_since_recent_inq'].fillna(-1)\n",
    "    test_data['revol_util'] = test_data['revol_util'].fillna(-1)\n",
    "    test_data['total_bc_limit'] = test_data['total_bc_limit'].fillna(-1)\n",
    "    test_data['mths_since_last_major_derog'] = test_data['mths_since_last_major_derog'].fillna(-1)\n",
    "    test_data['tot_hi_cred_lim'] = test_data['tot_hi_cred_lim'].fillna(-1)\n",
    "    test_data['bc_util'] = test_data['bc_util'].fillna(-1)\n",
    "    test_data['tot_cur_bal'] = test_data['tot_cur_bal'].fillna(-1)\n",
    "\n",
    "    # Generate embeddings for the text column\n",
    "    test_data['desc_clean'] = test_data[text_column].fillna('').apply(clean_text)\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    embeddings = model.encode(test_data['desc_clean'].tolist(), show_progress_bar=True)\n",
    "    embedding_df = pd.DataFrame(embeddings, columns=[f\"emb_{i}\" for i in range(embeddings.shape[1])])\n",
    "    test_with_embeddings = pd.concat([test_data.reset_index(drop=True), embedding_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    test_with_embeddings.drop(columns=exclude_cols + [text_column, 'target'], errors='ignore', inplace=True)\n",
    "\n",
    "    # Encode non-numeric columns\n",
    "    non_numeric_cols = test_with_embeddings.select_dtypes(include=['object', 'category']).columns\n",
    "    label_encoder = LabelEncoder()\n",
    "    for col in non_numeric_cols:\n",
    "        test_with_embeddings[col] = label_encoder.fit_transform(test_with_embeddings[col].astype(str))\n",
    "\n",
    "    # Scale the test data using the scaler fitted on the training data\n",
    "    test_scaled = scaler.transform(test_with_embeddings)\n",
    "\n",
    "    return test_scaled\n",
    "\n",
    "\n",
    "\n",
    "# Wrap it all together\n",
    "def main(train_data, test_data):\n",
    "    # Preprocess data\n",
    "    train_loader, val_loader, input_size, scaler= preprocess_data(train_data)\n",
    "\n",
    "    model = train_model(train_loader, val_loader, input_size)\n",
    "\n",
    "    # Load the best model\n",
    "    best_model = load_best_model(input_size, hidden_size=64)\n",
    "\n",
    "    # Preprocess test data\n",
    "    test_scaled = preprocess_test_data(test_data, scaler=scaler)  # Pass the scaler used for training\n",
    "\n",
    "    # Predict on test data\n",
    "    test_predictions = predict(best_model, test_scaled)\n",
    "\n",
    "    return test_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a99bfda0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tp/k1f107px41d6gt437332cg440000gn/T/ipykernel_29032/3824023693.py:2: DtypeWarning: Columns (8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  test_data = pd.read_csv('./testing_loan_data.csv')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28b255902e5d47ee812a1f6df8007deb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5921 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(189457, 384)\n",
      "Columns with missing values:\n",
      "Index([], dtype='object')\n",
      "Finish preprocessing\n",
      "Start Training\n",
      "Epoch [1/50], Train Loss: 0.2659, Val Loss: 0.2542, ROC-AUC: 0.6308, Learning Rate: 0.010000\n",
      "Epoch [2/50], Train Loss: 0.2601, Val Loss: 0.2511, ROC-AUC: 0.6438, Learning Rate: 0.010000\n",
      "Epoch [3/50], Train Loss: 0.2561, Val Loss: 0.2619, ROC-AUC: 0.6522, Learning Rate: 0.010000\n",
      "Epoch [4/50], Train Loss: 0.2533, Val Loss: 0.2590, ROC-AUC: 0.6425, Learning Rate: 0.010000\n",
      "Epoch [5/50], Train Loss: 0.2509, Val Loss: 0.2614, ROC-AUC: 0.6161, Learning Rate: 0.001000\n",
      "Epoch [6/50], Train Loss: 0.2393, Val Loss: 0.2456, ROC-AUC: 0.6757, Learning Rate: 0.001000\n",
      "Epoch [7/50], Train Loss: 0.2365, Val Loss: 0.2440, ROC-AUC: 0.6768, Learning Rate: 0.001000\n",
      "Epoch [8/50], Train Loss: 0.2349, Val Loss: 0.2457, ROC-AUC: 0.6784, Learning Rate: 0.001000\n",
      "Epoch [9/50], Train Loss: 0.2330, Val Loss: 0.2451, ROC-AUC: 0.6754, Learning Rate: 0.001000\n",
      "Epoch [10/50], Train Loss: 0.2316, Val Loss: 0.2485, ROC-AUC: 0.6762, Learning Rate: 0.000100\n",
      "Epoch [11/50], Train Loss: 0.2267, Val Loss: 0.2469, ROC-AUC: 0.6740, Learning Rate: 0.000100\n",
      "Epoch [12/50], Train Loss: 0.2256, Val Loss: 0.2478, ROC-AUC: 0.6736, Learning Rate: 0.000100\n",
      "Epoch [13/50], Train Loss: 0.2249, Val Loss: 0.2493, ROC-AUC: 0.6742, Learning Rate: 0.000100\n",
      "Early stopping triggered after 13 epochs.\n",
      "Final Validation ROC-AUC: 0.6784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tp/k1f107px41d6gt437332cg440000gn/T/ipykernel_29032/802363388.py:160: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_model.pth'))\n",
      "/var/folders/tp/k1f107px41d6gt437332cg440000gn/T/ipykernel_29032/802363388.py:195: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68feead5cc2e42e3b79eaf6019e1bf24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/3204 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to 'test_predictions.csv'\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv('./training_loan_data.csv',header = 1)\n",
    "test_data = pd.read_csv('./testing_loan_data.csv')\n",
    "\n",
    "test_predictions = main(train_data, test_data)\n",
    "\n",
    "# Save predictions\n",
    "import pandas as pd\n",
    "submission = pd.DataFrame({'prediction': test_predictions})\n",
    "submission.to_csv('test_predictions.csv', index=False)\n",
    "\n",
    "print(\"Predictions saved to 'test_predictions.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ce2c30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
